{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en la carpeta raw: ['test.csv', 'train.csv', 'transactions.csv', 'oil.csv', 'holidays_events.csv', 'sample_submission.csv', 'stores.csv']\n",
      "Los valores nulos en holidays.date son: 0\n",
      " HOLIDAY: RANGO DE FECHAS DE 2012-03-02 00:00:00 a 2017-12-26 00:00:00\n",
      "Los datos iniciales son: 350 los que hemos quitado 12 los que nos quedan 338  \n",
      "Los valores nulos en oil.date son: 0\n",
      "Los valores nulos en stores.store_nbr son: 0\n",
      "Los valores nulos en transactions.store_nbr son: 0\n",
      "Los valores nulos en transactions.date son: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Ruta desde notebooks/ hacia los CSV\n",
    "data_path = \"../data/raw/\"\n",
    "\n",
    "# Listar archivos para confirmar que encuentra los CSV\n",
    "archivos = os.listdir(data_path)\n",
    "print(\"Archivos en la carpeta raw:\", archivos)\n",
    "\n",
    "#HOLIDAYS_EVENTS\n",
    "holiday_df = pd.read_csv(os.path.join(data_path, \"holidays_events.csv\"))\n",
    "\n",
    "#CONTROL DE FECHAS HOLIDAY\n",
    "\n",
    "holiday_df[\"date\"] = pd.to_datetime(holiday_df[\"date\"])\n",
    "\n",
    "fecha_min = holiday_df[\"date\"].min()\n",
    "fecha_max = holiday_df[\"date\"].max()\n",
    " \n",
    "nulos_date_holidays = holiday_df[\"date\"].isnull().sum()\n",
    "\n",
    "print(f\"Los valores nulos en holidays.date son: {nulos_date_holidays}\")\n",
    "print(f\" HOLIDAY: RANGO DE FECHAS DE {fecha_min} a {fecha_max}\")\n",
    "\n",
    "\n",
    "holiday_df[\"date\"] = pd.to_datetime(holiday_df[\"date\"])  # Asegurar que es datetime\n",
    "holiday_df[\"year\"] = holiday_df[\"date\"].dt.year\n",
    "holiday_df[\"month\"] = holiday_df[\"date\"].dt.month\n",
    "holiday_df[\"day\"] = holiday_df[\"date\"].dt.day\n",
    "holiday_df[\"day_of_week\"] = holiday_df[\"date\"].dt.weekday  # 0 = Lunes, 6 = Domingo\n",
    "holiday_df[\"week_of_year\"] = holiday_df[\"date\"].dt.isocalendar().week  # Semana del a√±o\n",
    "\n",
    "#LIMPIEZA DE LA COLUMNA TRANSFERRED:\n",
    "\n",
    "# Filtrar solo los registros donde transferred == False (es decir, NO transferidos)\n",
    "holiday_df_clean = holiday_df[holiday_df.transferred == False]\n",
    "\n",
    "# Eliminar la columna transferred, ya que ahora es innecesaria\n",
    "holiday_df_clean = holiday_df_clean.drop(\"transferred\", axis=1)\n",
    "\n",
    "# Reiniciar el √≠ndice tras la limpieza\n",
    "holiday_df_clean = holiday_df_clean.reset_index(drop=True)\n",
    "\n",
    "#COMPROBACI√ìN DE QUE ES CORRECTA\n",
    "\n",
    "datos_iniciales = holiday_df.shape[0]\n",
    "\n",
    "datosSinTransferred= holiday_df[holiday_df[\"transferred\"] != False].shape[0]\n",
    "\n",
    "\n",
    "holiday_df_clean = holiday_df[holiday_df[\"transferred\"] == False]\n",
    "\n",
    "datos_postFiltrado = holiday_df_clean.shape[0]\n",
    "\n",
    "print(f\"Los datos iniciales son: {datos_iniciales} los que hemos quitado {datosSinTransferred} los que nos quedan {datos_postFiltrado}  \")\n",
    "\n",
    "nulos_date_holidays_clean = holiday_df_clean[\"date\"].isnull().sum()\n",
    "\n",
    "\n",
    "if nulos_date_holidays_clean > 0:\n",
    "    raise ValueError(f\"‚ùå ERROR: Existen {nulos_date_holidays_clean} valores nulos en la columna 'date' de holiday_df\")\n",
    "\n",
    "duplicados_holidays= holiday_df_clean.duplicated(subset=[\"date\", \"type\", \"locale\", \"locale_name\",\"description\"], keep=False)\n",
    "\n",
    "if duplicados_holidays.any():\n",
    "    raise ValueError(f\"‚ùå ERROR: Existen {duplicados_holidays} valores duplicados en la clave √∫nica de holiday_df\")\n",
    "#OIL \n",
    "\n",
    "# Cargar y mostrar las primeras filas del archivo oil.csv\n",
    "oil_df = pd.read_csv(os.path.join(data_path, \"oil.csv\"))\n",
    "\n",
    "\n",
    "oil_df[\"date\"] = pd.to_datetime(oil_df[\"date\"])\n",
    "\n",
    "#HAY NULOS EN DATE?\n",
    "\n",
    "nulos_date_oil = oil_df[\"date\"].isnull().sum()\n",
    "\n",
    "print(f\"Los valores nulos en oil.date son: {nulos_date_oil}\")\n",
    "\n",
    "\n",
    "if nulos_date_oil > 0:\n",
    "    raise ValueError(f\"‚ùå ERROR: Existen {nulos_date_oil} valores nulos en la columna 'date' de oil_df\")\n",
    "\n",
    "duplicados_oil= oil_df.duplicated(subset=[\"date\"])\n",
    "\n",
    "if duplicados_oil.any():\n",
    "    raise ValueError(f\"‚ùå ERROR: Existen {duplicados_oil} valores duplicados en la clave √∫nica de oil\")\n",
    "\n",
    "oil_df = oil_df.fillna(-1)  # Reemplaza NaN por -1\n",
    "\n",
    "\n",
    "# STORES\n",
    "stores = pd.read_csv(os.path.join(data_path, \"stores.csv\"))\n",
    "\n",
    "store_nbr = stores[\"store_nbr\"].isnull().sum()\n",
    "\n",
    "print(f\"Los valores nulos en stores.store_nbr son: {store_nbr}\")\n",
    "\n",
    "if store_nbr > 0:\n",
    "    raise ValueError(f\"‚ùå ERROR: Existen {store_nbr} valores nulos en la columna 'store_nbr' de stores\")\n",
    "\n",
    "duplicados_store = stores.duplicated(subset=[\"store_nbr\"])\n",
    "\n",
    "if duplicados_store.any():\n",
    "    raise ValueError(f\"‚ùå ERROR: Existen {duplicados_store} valores duplicados en la clave √∫nica de store\")\n",
    "\n",
    "#Transactions\n",
    "\n",
    "transactions = pd.read_csv(os.path.join(data_path, \"transactions.csv\"))\n",
    "\n",
    "transactions[\"date\"] = pd.to_datetime(transactions[\"date\"])\n",
    "\n",
    "transactions_storenbr = transactions[\"store_nbr\"].isnull().sum()\n",
    "transactions_date = transactions[\"date\"].isnull().sum()\n",
    "\n",
    "print(f\"Los valores nulos en transactions.store_nbr son: {transactions_storenbr}\")\n",
    "print(f\"Los valores nulos en transactions.date son: {transactions_date}\")\n",
    "\n",
    "if ((transactions_storenbr > 0) and (transactions_date > 0)):\n",
    "    raise ValueError(f\"‚ùå ERROR: Existen {transactions_storenbr} valores nulos en la columna 'store_nbr' de transactions y Existen {transactions_date} valores nulos en la columna 'date' de transactions\")\n",
    "\n",
    "duplicados_transactions= transactions.duplicated(subset=[\"store_nbr\",\"date\"])\n",
    "\n",
    "if duplicados_transactions.any():\n",
    "    raise ValueError(f\"‚ùå ERROR: Existen {duplicados_transactions} valores duplicados en la clave √∫nica de transactions\")\n",
    "\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  \n",
    "processed_path = os.path.join(base_dir,\"data\",\"processed\")\n",
    "\n",
    "holiday_df_clean.to_csv(os.path.join(processed_path,\"holiday_events.csv\"),index=False)\n",
    "oil_df.to_csv(os.path.join(processed_path,\"oil.csv\"),index=False)\n",
    "stores.to_csv(os.path.join(processed_path,\"stores.csv\"),index=False)\n",
    "transactions.to_csv(os.path.join(processed_path,\"transactions.csv\"),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacemos las querys recorriendo los distintos df y lo metemos en el script de arriba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos insertados correctamente en la tabla holiday_event.\n",
      "‚úÖ Datos insertados correctamente en la tabla transactions.\n",
      "‚úÖ Datos insertados correctamente en la tabla stores.\n",
      "‚úÖ Datos insertados correctamente en la tabla oil.\n",
      "üîå Conexi√≥n a MySQL cerrada.\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Conectar con MySQL\n",
    "conn = pymysql.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"\",  # Pon tu contrase√±a si la tienes\n",
    "    database=\"store_sales\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Definir ruta base y archivos\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "processed_path = os.path.join(base_dir, \"data\", \"processed\")\n",
    "\n",
    "files = {\n",
    "    \"holiday_event\": \"holiday_events.csv\",\n",
    "    \"transactions\": \"transactions.csv\",\n",
    "    \"stores\": \"stores.csv\",\n",
    "    \"oil\": \"oil.csv\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    for table, filename in files.items():\n",
    "        file_path = os.path.join(processed_path, filename)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"‚ùå ERROR: No se encontr√≥ el archivo {filename} en {file_path}\")\n",
    "\n",
    "        df = pd.read_csv(file_path)  # Cargar CSV din√°micamente\n",
    "\n",
    "        # Construcci√≥n de la consulta seg√∫n la tabla\n",
    "        if table == \"holiday_events\":\n",
    "            query = \"\"\"\n",
    "            INSERT IGNORE INTO holiday_events (date, type, locale, locale_name, description, transferred, year, month, day, day_week, week_year)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            data = [\n",
    "                (\n",
    "                    row[\"date\"], row[\"type\"], row[\"locale\"], row[\"locale_name\"], row[\"description\"],\n",
    "                    row[\"transferred\"], row[\"year\"], row[\"month\"], row[\"day\"],\n",
    "                    row[\"day_of_week\"], row[\"week_of_year\"]\n",
    "                ) for _, row in df.iterrows()\n",
    "            ]\n",
    "        elif table == \"transactions\":\n",
    "            query = \"\"\"\n",
    "            INSERT IGNORE INTO transactions (date, store_nbr, transactions)\n",
    "            VALUES (%s, %s, %s)\n",
    "            \"\"\"\n",
    "            data = [(row[\"date\"], row[\"store_nbr\"], row[\"transactions\"]) for _, row in df.iterrows()]\n",
    "\n",
    "        elif table == \"stores\":\n",
    "            query = \"\"\"\n",
    "            INSERT IGNORE INTO stores (store_nbr, city, state, type, cluster)\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            data = [(row[\"store_nbr\"], row[\"city\"], row[\"state\"], row[\"type\"], row[\"cluster\"]) for _, row in df.iterrows()]\n",
    "\n",
    "        elif table == \"oil\":\n",
    "            query = \"\"\"\n",
    "            INSERT IGNORE INTO oil (date, price)\n",
    "            VALUES (%s, %s)\n",
    "            \"\"\"\n",
    "            data = [(row[\"date\"], row[\"dcoilwtico\"]) for _, row in df.iterrows()]\n",
    "\n",
    "        # Iniciar transacci√≥n e insertar datos\n",
    "        try:\n",
    "            conn.begin()\n",
    "            cursor.executemany(query, data)\n",
    "            conn.commit()\n",
    "            print(f\"‚úÖ Datos insertados correctamente en la tabla {table}.\")\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            raise ValueError(f\"‚ùå ERROR al insertar en {table}: {e}\")\n",
    "except Exception as general_error:\n",
    "   raise ValueError(f\"‚ùå ERROR: Existen {general_error} valores nulos en la columna 'date' de oil_df\")\n",
    "\n",
    "finally:\n",
    "    # Cerrar cursor y conexi√≥n para liberar recursos\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"üîå Conexi√≥n a MySQL cerrada.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
